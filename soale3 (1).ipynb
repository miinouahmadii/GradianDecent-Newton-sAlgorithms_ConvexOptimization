{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "soale3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from sympy import symbols, Eq, solve, log\n",
        "\n",
        "def partial(element, function):\n",
        "\t\"\"\"\n",
        "\tpartial : sympy.core.symbol.Symbol * sympy.core.add.Add -> sympy.core.add.Add\n",
        "\tpartial(element, function) Performs partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant. Return partial_diff.\n",
        "\t\"\"\"\n",
        "\tpartial_diff = function.diff(element)\n",
        "\n",
        "\treturn partial_diff\n",
        "\n",
        "\n",
        "def gradient(partials):\n",
        "\t\"\"\"\n",
        "\tgradient : List[sympy.core.add.Add] -> numpy.matrix\n",
        "\tgradient(partials) Transforms a list of sympy objects into a numpy matrix. Return grad.\n",
        "\t\"\"\"\n",
        "\tgrad = numpy.matrix([[partials[0]], [partials[1]]])\n",
        "\n",
        "\treturn grad\n",
        "def gradient_to_zero(symbols_list, partials):\n",
        "\t\"\"\"\n",
        "\tgradient_to_zero : List[sympy.core.symbol.Symbol] * List[sympy.core.add.Add] -> Dict[sympy.core.numbers.Float]\n",
        "\tgradient_to_zero(symbols_list, partials) Solve the null equation for each variable, and determine the pair of coordinates of the singular point. Return singular.\n",
        "\t\"\"\"\n",
        "\tpartial_x = Eq(partials[0], 0)\n",
        "\tpartial_y = Eq(partials[1], 0)\n",
        "\n",
        "\tsingular = solve((partial_x, partial_y), (symbols_list[0], symbols_list[1]))\n",
        "\n",
        "\treturn singular\n",
        "\n",
        "def determat(partials_second, cross_derivatives, singular, symbols_list):\n",
        "\t\"\"\"\n",
        "\tList[sympy.core.add.Add] * sympy.core.add.Add * Dict[sympy.core.numbers.Float] * List[sympy.core.symbol.Symbol] -> sympy.core.numbers.Float\n",
        "\tdetermat(partials_second, cross_derivatives, singular, symbols_list) Computes the determinant of the Hessian matrix at the singular point. Return det.\n",
        "\t\"\"\"\n",
        "\tdet = partials_second[0].subs([(symbols_list[0], singular[symbols_list[0]]), (symbols_list[1], singular[symbols_list[1]])]) * partials_second[1].subs([(symbols_list[0], singular[symbols_list[0]]), (symbols_list[1], singular[symbols_list[1]])]) - (cross_derivatives.subs([(symbols_list[0], singular[symbols_list[0]]), (symbols_list[1], singular[symbols_list[1]])]))**2\n",
        "\n",
        "\treturn det\n",
        "\n",
        "def hessian(partials_second, cross_derivatives):\n",
        "\t\"\"\"\n",
        "\thessian : List[sympy.core.add.Add] * sympy.core.add.Add -> numpy.matrix\n",
        "\thessian(partials_second, cross_derivatives) Transforms a list of sympy objects into a numpy hessian matrix. Return hessianmat.\n",
        "\t\"\"\"\n",
        "\thessianmat = numpy.matrix([[partials_second[0], cross_derivatives], [cross_derivatives, partials_second[1]]])\n",
        "\n",
        "\treturn hessianmat\n",
        "  \n",
        "def main():\n",
        "\t\"\"\"\n",
        "\tFonction principale.\n",
        "\t\"\"\"\n",
        "\tx, y = symbols('x y')\n",
        "\tsymbols_list = [x, y]\n",
        "\tfunction = x**4 + (y-1)**2\n",
        "\tpartials, partials_second = [], []\n",
        "\n",
        "\tfor element in symbols_list:\n",
        "\t\tpartial_diff = partial(element, function)\n",
        "\t\tpartials.append(partial_diff)\n",
        "\n",
        "\tgrad = gradient(partials)\n",
        "\tsingular = gradient_to_zero(symbols_list, partials)\n",
        "\n",
        "\tcross_derivatives = partial(symbols_list[0], partials[1])\n",
        "\n",
        "\tfor i in range(0, len(symbols_list)):\n",
        "\t\tpartial_diff = partial(symbols_list[i], partials[i])\n",
        "\t\tpartials_second.append(partial_diff)\n",
        "\n",
        "\thessianmat = hessian(partials_second, cross_derivatives)\n",
        "\tdet = determat(partials_second, cross_derivatives, singular, symbols_list)\n",
        "\n",
        "\tprint(\"Hessian matrix that organizes all the second partial derivatives of the function {0} is :\\n {1}\".format(function, hessianmat))\n",
        "\tprint(\"Determinant in the singular point {0} is :\\n {1}\".format(singular, det))\n",
        " \n",
        "print(\"the function is convex\")\n",
        "print(\"Hessian matrix is positive definite matrix\")\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSObNZ0NvBZZ",
        "outputId": "3dfbefc6-900f-45d0-af28-bb108ffecc8d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the function is convex\n",
            "Hessian matrix is positive definite matrix\n",
            "Hessian matrix that organizes all the second partial derivatives of the function x**4 + (y - 1)**2 is :\n",
            " [[12*x**2 0]\n",
            " [0 2]]\n",
            "Determinant in the singular point {x: 0, y: 1} is :\n",
            " 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "multivariable gradient decent with exact line search"
      ],
      "metadata": {
        "id": "QdKi9cPWw1BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cur_x = 1 # The algorithm starts at x=1\n",
        "precision = 0.0001 #This tells us when to stop the algorithm\n",
        "previous_step_size = 1 #\n",
        "max_iters = 1000 # maximum number of iterations\n",
        "iters = 0 #iteration counter\n",
        "df = lambda x: 4 * x**3 #Gradient of our function\n",
        "\n",
        "while previous_step_size > precision and iters < max_iters:\n",
        "    prev_x = cur_x #Store current x value in prev_x\n",
        "    cur_x = cur_x - 1/(5 * cur_x**2) * df(prev_x) #Grad descent\n",
        "    previous_step_size = abs(cur_x - prev_x) #Change in x\n",
        "    iters = iters+1 #iteration count\n",
        "\n",
        "    print(\"Iteration\",iters,\"\\nX value is\",cur_x) #Print iterations\n",
        "    \n",
        "print(\"The local minimum occurs at\", cur_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJyuR5QTxCml",
        "outputId": "ea3a9c12-6b29-4915-cfa6-2670f582a295"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 \n",
            "X value is 0.19999999999999996\n",
            "Iteration 2 \n",
            "X value is 0.03999999999999998\n",
            "Iteration 3 \n",
            "X value is 0.007999999999999993\n",
            "Iteration 4 \n",
            "X value is 0.001599999999999999\n",
            "Iteration 5 \n",
            "X value is 0.00031999999999999976\n",
            "Iteration 6 \n",
            "X value is 6.399999999999993e-05\n",
            "Iteration 7 \n",
            "X value is 1.2799999999999993e-05\n",
            "The local minimum occurs at 1.2799999999999993e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "multivariable gradient decent with backtracking line search"
      ],
      "metadata": {
        "id": "QMr2fr7sRDUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cur_x = 1 # The algorithm starts at x=3\n",
        "rate = 1 # Learning rate\n",
        "beta = 0.5\n",
        "precision = 0.0001 #This tells us when to stop the algorithm\n",
        "previous_step_size = 1 #\n",
        "max_iters = 1000 # maximum number of iterations\n",
        "iters = 0 #iteration counter\n",
        "df = lambda x: 4 * x**3 #Gradient of our function\n",
        "\n",
        "while previous_step_size > precision and iters < max_iters:\n",
        "    prev_x = cur_x #Store current x value in prev_x\n",
        "    rate *= beta\n",
        "    cur_x = cur_x - rate * df(prev_x) #Grad descent with backtracking line search\n",
        "    print(rate)\n",
        "    previous_step_size = abs(cur_x - prev_x) #Change in x\n",
        "    iters = iters+1 #iteration count\n",
        "\n",
        "    print(\"Iteration\",iters,\"\\nX value is\",cur_x) #Print iterations\n",
        "    \n",
        "\n",
        "print(\"The local minimum occurs at\", cur_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7mh5HEQDmNW",
        "outputId": "d8587f0c-b086-4bc3-cd9a-bb423de27e49"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            "Iteration 1 \n",
            "X value is -1.0\n",
            "0.25\n",
            "Iteration 2 \n",
            "X value is 0.0\n",
            "0.125\n",
            "Iteration 3 \n",
            "X value is 0.0\n",
            "The local minimum occurs at 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "multivariable Newton's algorithm with exact line search"
      ],
      "metadata": {
        "id": "bgRtzHJkRT23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cur_x = 1 # The algorithm starts at x=3\n",
        "precision = 0.0001 #This tells us when to stop the algorithm\n",
        "previous_step_size = 1 #\n",
        "max_iters = 1000 # maximum number of iterations\n",
        "iters = 0 #iteration counter\n",
        "df = lambda x: 4 * x**3 #Gradient of our function\n",
        "df2 = lambda x:12 * x**2 #Hessian  of our function\n",
        "while previous_step_size > precision and iters < max_iters:\n",
        "    prev_x = cur_x #Store current x value in prev_x\n",
        "    cur_x = cur_x - 1/(49 * cur_x**4) * df2(prev_x) * df(prev_x) #Newton's algorithm with exact line search\n",
        "    previous_step_size = abs(cur_x - prev_x) #Change in x\n",
        "    iters = iters+1 #iteration count\n",
        "\n",
        "    print(\"Iteration\",iters,\"\\nX value is\",cur_x) #Print iterations\n",
        "    \n",
        "print(\"The local minimum occurs at\", cur_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aD4jduXEtMd",
        "outputId": "33c1352e-2878-477c-e57b-999a8dbb9dfd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 \n",
            "X value is 0.020408163265306145\n",
            "Iteration 2 \n",
            "X value is 0.00041649312786338696\n",
            "Iteration 3 \n",
            "X value is 8.499859752314075e-06\n",
            "Iteration 4 \n",
            "X value is 1.7346652555742955e-07\n",
            "The local minimum occurs at 1.7346652555742955e-07\n"
          ]
        }
      ]
    }
  ]
}